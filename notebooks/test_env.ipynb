{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37980a2e",
   "metadata": {},
   "source": [
    "# Test ForagerEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f75901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77258490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.foragers import ForagersEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce41eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn=0  State=(0.5, 0.5) Manager budget=0.10 Forager budget=0.10\n"
     ]
    }
   ],
   "source": [
    "env = ForagersEnv(\n",
    "    initial_rate=0.5,\n",
    "    initial_wealth=0.5,\n",
    "    num_foragers=3,\n",
    ")\n",
    "state = env.reset()\n",
    "env.debug = True  # Enable debug prints\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14191f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manager invests: False\n",
      "Forager goes foraging: True\n",
      "Total harvest: 0.0\n",
      "Manager budget after update: 0.1\n",
      "Forager budget after update: 0.0\n",
      "New wealth before clipping: 0.1\n",
      "New wealth after clipping: 0.1\n",
      "Reward calculation: Wealth=0.1, Inequality penalty=0.10000000000000002\n",
      "New state: (0.2, np.float64(0.1)), Reward: -1.3877787807814457e-17, Done: True\n"
     ]
    }
   ],
   "source": [
    "action = 0.2\n",
    "new_state, reward, done, _, _ = env.step(action)\n",
    "print(f'New state: {new_state}, Reward: {reward}, Done: {done}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c6ab7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3596053193.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from utils.interaction import Episode\n",
    "from utils.interpreters import gym_interpreter1\n",
    "from "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b8f35",
   "metadata": {},
   "source": [
    "Create Tile-coding state-value function approximator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676df82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_scales = [\n",
    "    {'min':0, 'max':1},\n",
    "    {'min':0, 'max':1},\n",
    "]\n",
    "tiles_parameters = {\n",
    "    'numDims': 2,\n",
    "    'numTilings': 8,\n",
    "    'numTiles': [8, 8],\n",
    "    'scaleFactors':state_scales,\n",
    "    'maxSize':4096,\n",
    "    'alpha':0.1\n",
    "}\n",
    "value_approximator = TilesQ(parameters=tiles_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7d8ea",
   "metadata": {},
   "source": [
    "Create Q-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a18bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_parameters = {\n",
    "    'nA':3,\n",
    "    'nS':2,\n",
    "    'gamma':1,\n",
    "    'epsilon':0,\n",
    "    'alpha':0.1,\n",
    "    'Q':value_approximator\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6448cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_parameters = {\n",
    "    'nA':4,\n",
    "    'nS':16,\n",
    "    'gamma':1,\n",
    "    'epsilon':0.1,\n",
    "    'alpha':0.1\n",
    "}\n",
    "env_parameters = {'desc':[\"SFFF\", \"FFHF\", \"FHFF\", \"FFFG\"],\n",
    "                  'is_slippery':True}\n",
    "perf = Performer(env_name='FrozenLake-v1',\n",
    "                 env_parameters=env_parameters,\n",
    "                 state_interpreter=gym_interpreter1,\n",
    "                 agent_name='Q_learning',\n",
    "                 agent_parameters=agent_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205d9d6",
   "metadata": {},
   "source": [
    "Para verificar que el agente y el entorno están interactuando correctamente, podemos usar el método `run()`, el cual mostrará al agente actuando sobre el entorno. Dado que el render del entorno ABC es muy poco informativo, pues solo muestra el estado en el que está el agente, vamos a correr en modo para cazar errores usando el parámetro `visual=False`. Usualmente corremos este método sin argumentos, pues por defecto el valor es `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf.run(from_file=False, to_video=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b992ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf.environment.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab11f5",
   "metadata": {},
   "source": [
    "Corremos el método de entrenamiento, `train()`, para decirle al agente que aprenda a resolver el entorno. Observe que se obtiene un dataset con el progreso del agente y una gráfica de la recompensa total por episodio. Si el agente está aprendiendo, la recompensa debería aumentar progresivamente. El agente (política y valores Q) se guarda en un archivo json al final del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e41b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf.train(\n",
    "    num_rounds=1000,\n",
    "    num_episodes=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ffab4",
   "metadata": {},
   "source": [
    "Si queremos ver el desempeño del agente \"en producción\", es decir, sin que este pueda deambular explorando acciones aleatorias, podemos usar el método `test()`. Obtendremos el promedio de la recompensa total sobre 100 episodios y el correspondiente histograma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd22e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf.test(num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f14c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf.run(from_file=True, to_video=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
